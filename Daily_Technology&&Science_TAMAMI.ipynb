{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Daily-Technology&&Science_TAMAMI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7pWgBm2BKdo5wF9O4QKlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doukansurel/Kod/blob/main/Daily_Technology%26%26Science_TAMAMI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cFto8xqfESaX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import re\n",
        "import urllib3\n",
        "from pandas import DataFrame\n",
        "import csv\n",
        "import datetime\n",
        "from datetime import datetime, timedelta ,date"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "technology_link = []\n",
        "science_link = []"
      ],
      "metadata": {
        "id": "xhRjaJJaEmPi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def date_creator():\n",
        "  for i in range(1,234):\n",
        "    lnk = \"https://www.dailysabah.com/search?query=Technology&qlimit=by_fifty&pgno={}\".format(str(i))\n",
        "    technology_link.append(lnk)\n",
        "\n",
        "  for i in range(1,170):\n",
        "    lnk = \"https://www.dailysabah.com/search?qsubsection=science&pgno={}\".format(str(i))\n",
        "    science_link.append(lnk)"
      ],
      "metadata": {
        "id": "5cQEDAMwEsqc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_creator()"
      ],
      "metadata": {
        "id": "GWHbqDdIGuhS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(technology_link) #11613 tane haber var\n",
        "print(science_link) #1684 tane haber var"
      ],
      "metadata": {
        "id": "sEZTUHPHF8zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technology_wrong_link = []"
      ],
      "metadata": {
        "id": "rRorXidZsA9s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def science_get_link_creator(url):\n",
        "  html = requests.get(url).content                                                          \n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  list = soup.find(\"div\",{\"class\" :\"list_with_rectangle_image search_results\"}).find_all(\"div\",{\"class\": \"widget_content\"})\n",
        "  for i in list:\n",
        "    href = i.find(\"h3\").find(\"a\").get(\"href\")\n",
        "    with open(\"daily_science_Links.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "      file.write(href+\"\\n\")\n",
        "    \n"
      ],
      "metadata": {
        "id": "GEaXaeFKaY0h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def technology_get_link_creator(url):\n",
        "  html = requests.get(url).content                                                          \n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  list = soup.find(\"div\",{\"class\" :\"list_with_rectangle_image search_results\"}).find_all(\"div\",{\"class\": \"widget_content\"})\n",
        "  for i in list:\n",
        "    href = i.find(\"h3\").find(\"a\").get(\"href\")\n",
        "    if(href.startswith(\"https://www.dailysabah.com/life/science/\")):\n",
        "      technology_wrong_link.append(href)\n",
        "    else :   \n",
        "      with open(\"daily_technology_Links.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "        file.write(href+\"\\n\")"
      ],
      "metadata": {
        "id": "PZmDP32fo_kB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(science_get_link_creator,i.strip()) for i in science_link]\n",
        "print(time.time()-t1)\n",
        "\n",
        "t2=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(technology_get_link_creator,i.strip()) for i in technology_link]\n",
        "print(time.time()-t2)"
      ],
      "metadata": {
        "id": "wZ5aTy07tcEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technology_links = []\n",
        "science_links = []"
      ],
      "metadata": {
        "id": "q4jxUfvV5oPb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"daily_technology_Links.txt\",'r',newline='') as f:\n",
        "  for i in f.readlines():    \n",
        "  # reader=csv.reader(f)\n",
        "    i = i.strip(\"\\n\")\n",
        "    technology_links.append(i)"
      ],
      "metadata": {
        "id": "A87ztrRG5kAO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"daily_science_Links.txt\",'r',newline='') as f:\n",
        "  for i in f.readlines():    \n",
        "  # reader=csv.reader(f)\n",
        "    i = i.strip(\"\\n\")\n",
        "\n",
        "    science_links.append(i)"
      ],
      "metadata": {
        "id": "-KsI0TNf5zej"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(technology_links)\n",
        "print(science_links)"
      ],
      "metadata": {
        "id": "mA1U4fug66mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creator_science_content(url):\n",
        "  html = requests.get(url).content\n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  title = soup.find(\"div\",{\"class\" : \"top_title_widget\"}).find(\"h1\").getText().strip() # title tamam \n",
        "  content = soup.find(\"div\",{\"class\" : \"article_body\"})\n",
        "  date = soup.find(\"div\",{\"class\" : \"left_mobile_details\"})\n",
        "  categori = \"Science\"\n",
        "  content_string=\"\"\n",
        "  if date == None:\n",
        "    date = \"\"\n",
        "  else :\n",
        "    date =soup.find(\"div\",{\"class\" : \"left_mobile_details\"}).find_all(\"span\")\n",
        "    if(len(date) == 2 ):\n",
        "        date = date[1].getText().strip()\n",
        "        date = date[:-15].replace(\",\",\"\") \n",
        "        date = date.strip()\n",
        "    else : \n",
        "        date = date[2].getText().strip()\n",
        "        date = date[:-16].replace(\",\",\"\") \n",
        "        date = date.strip()\n",
        "    \n",
        "  if content == None : \n",
        "    sentence = \"{} ; {} ; {} ; {} ; \".format(url,date,categori,title)\n",
        "    with open(\"daily_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "      file.write(sentence+\"\\n\")\n",
        "  else:\n",
        "    content = soup.find(\"div\",{\"class\" : \"article_body\"}).find_all(\"p\")\n",
        "    for i in content:\n",
        "      i=i.getText().replace(\" \",\" \")\n",
        "      z=\" \".join(i.split())        \n",
        "      content_string += z\n",
        "          \n",
        "    if content_string != \"\":\n",
        "      content = soup.find(\"div\",{\"class\" : \"article_body\"}).getText()  \n",
        "      content = content.strip()\n",
        "      content =\" \".join(content.split())\n",
        "      content_string = content   \n",
        "\n",
        "    sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,content_string)\n",
        "    with open(\"daily_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "      file.write(sentence+\"\\n\")\n"
      ],
      "metadata": {
        "id": "zTHehXnt6FFY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creator_technology_content(url):\n",
        "  html = requests.get(url).content\n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  title = soup.find(\"div\",{\"class\" : \"top_title_widget\"}).find(\"h1\").getText().strip() # title tamam \n",
        "  content = soup.find(\"div\",{\"class\" : \"article_body\"})\n",
        "  date = soup.find(\"div\",{\"class\" : \"left_mobile_details\"})\n",
        "  categori = \"Science\"\n",
        "  content_string=\"\"\n",
        "  if date == None:\n",
        "    date = \"\"\n",
        "  else :\n",
        "    date =soup.find(\"div\",{\"class\" : \"left_mobile_details\"}).find_all(\"span\")\n",
        "    if(len(date) == 2 ):\n",
        "        date = date[1].getText().strip()\n",
        "        date = date[:-15].replace(\",\",\"\") \n",
        "        date = date.strip()\n",
        "    else : \n",
        "        date = date[2].getText().strip()\n",
        "        date = date[:-16].replace(\",\",\"\") \n",
        "        date = date.strip()\n",
        "    \n",
        "  if content == None : \n",
        "    sentence = \"{} ; {} ; {} ; {} ; \".format(url,date,categori,title)\n",
        "    with open(\"daily_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "      file.write(sentence+\"\\n\")\n",
        "  else:\n",
        "    content = soup.find(\"div\",{\"class\" : \"article_body\"}).find_all(\"p\")\n",
        "    for i in content:\n",
        "      i=i.getText().replace(\" \",\" \")\n",
        "      z=\" \".join(i.split())        \n",
        "      content_string += z\n",
        "          \n",
        "    if content_string != \"\":\n",
        "      content = soup.find(\"div\",{\"class\" : \"article_body\"}).getText()  \n",
        "      content = content.strip()\n",
        "      content =\" \".join(content.split())\n",
        "      content_string = content   \n",
        "\n",
        "    sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,content_string)\n",
        "    with open(\"daily_technology_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "      file.write(sentence+\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3sm6fIZiwpiM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(creator_science_content,i.strip()) for i in science_links]\n",
        "print(time.time()-t1)\n",
        "\n",
        "t2=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(creator_technology_content,i.strip()) for i in technology_links]\n",
        "print(time.time()-t2)"
      ],
      "metadata": {
        "id": "4GDpYF_Nw1ys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}