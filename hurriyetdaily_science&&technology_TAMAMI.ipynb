{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hurriyetdaily_science&&technology_TAMAMI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfQDaWKHm2eL/a900bSyIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doukansurel/Kod/blob/main/hurriyetdaily_science%26%26technology_TAMAMI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vTi8_gmKBOSM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import re\n",
        "import urllib3\n",
        "from pandas import DataFrame\n",
        "import csv\n",
        "import datetime\n",
        "from datetime import datetime, timedelta ,date"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "technology_page_link = []\n",
        "science_page_link = []"
      ],
      "metadata": {
        "id": "lxyriB9xBkpz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def date_creator():\n",
        "  for i in range(1,51):\n",
        "    lnk = \"https://www.hurriyetdailynews.com/search/science?p={}\".format(str(i))\n",
        "    science_page_link.append(lnk)\n",
        "  for i in range(1,51):\n",
        "    lnk = \"https://www.hurriyetdailynews.com/search/technology?p={}\".format(str(i))\n",
        "    technology_page_link.append(lnk)"
      ],
      "metadata": {
        "id": "g_EMwnEmBlqb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_creator()"
      ],
      "metadata": {
        "id": "wSHVLNHpCqvC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(technology_page_link)\n",
        "print(science_page_link)"
      ],
      "metadata": {
        "id": "N209QSEkCsbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def technology_get_link_creator(url):\n",
        "  html = requests.get(url).content                                                          \n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  link_list = soup.find_all(\"div\",{\"class\" : \"col-md-4 col-sm-4\"})\n",
        "  for i in link_list:\n",
        "    href = i.find(\"a\").get(\"href\")\n",
        "    if href.startswith(\"https://www.hurriyetdailynews.com\"):\n",
        "      pass\n",
        "    else:\n",
        "      href = \"https://www.hurriyetdailynews.com\"+href\n",
        "      technology_links\n",
        "      with open(\"daily_hürriyet_technology_Link.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "        file.write(href+\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "o_C2-ocEDKgk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def science_get_link_creator(url):\n",
        "  html = requests.get(url).content                                                          \n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  link_list = soup.find_all(\"div\",{\"class\" : \"col-md-4 col-sm-4\"})\n",
        "  for i in link_list:\n",
        "    href = i.find(\"a\").get(\"href\")\n",
        "    if href.startswith(\"https://www.hurriyetdailynews.com\"):\n",
        "      pass\n",
        "    else:\n",
        "      href = \"https://www.hurriyetdailynews.com\"+href\n",
        "      technology_links\n",
        "      with open(\"daily_hürriyet_science_Link.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "        file.write(href+\"\\n\")"
      ],
      "metadata": {
        "id": "p9Q1mqDuGQXn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(science_get_link_creator,i.strip()) for i in science_page_link]\n",
        "print(time.time()-t1)\n",
        "\n",
        "t2=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(technology_get_link_creator,i.strip()) for i in technology_page_link]\n",
        "print(time.time()-t2)"
      ],
      "metadata": {
        "id": "0O94E6vYGeeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda778ef-95de-4277-f98d-3cd20f871ab7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28.196727752685547\n",
            "26.403637886047363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "technology_links = [] \n",
        "science_links = []\n"
      ],
      "metadata": {
        "id": "-LWEQ1726t-X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"daily_hürriyet_science_Link.txt\",'r',newline='') as f:\n",
        "  for i in f.readlines():    \n",
        "  # reader=csv.reader(f)\n",
        "    i = i.strip(\"\\n\")\n",
        "    science_links.append(i)\n",
        "\n",
        "with open(\"daily_hürriyet_technology_Link.txt\",'r',newline='') as f:\n",
        "  for i in f.readlines():    \n",
        "  # reader=csv.reader(f)\n",
        "    i = i.strip(\"\\n\")\n",
        "\n",
        "    technology_links.append(i)"
      ],
      "metadata": {
        "id": "E_YE1HVp7HtS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(technology_links)\n",
        "print(science_links)"
      ],
      "metadata": {
        "id": "TbyjMRUkALPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creator_science_content(url):\n",
        "  html = requests.get(url).content\n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  title = soup.find(\"div\",{\"class\": \"content\"}).find(\"h1\").getText() # TİTLE Tamam\n",
        "  date = soup.find(\"div\",{\"class\" : \"content-info\"}).find_all(\"ul\")\n",
        "  date = date[1].getText()\n",
        "\n",
        "  date = date[:-9]\n",
        "  date= date.strip()\n",
        "\n",
        "\n",
        "    \n",
        "  content = soup.find(\"div\",{\"class\" : \"content\"}).find_all(\"p\")\n",
        "  content_string =\"\"\n",
        "  categori =\"Science\"\n",
        "  \n",
        "  for i in content:\n",
        "    i=i.getText().replace(\" \",\" \")\n",
        "    z=\" \".join(i.split())        \n",
        "    content_string += z\n",
        "          \n",
        "\n",
        "  sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,content_string)\n",
        "  with open(\"hürriyetdaily_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "    file.write(sentence +\"\\n\")"
      ],
      "metadata": {
        "id": "-KfX7Frc7MRc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def creator_technology_content(url):\n",
        "  html = requests.get(url).content\n",
        "  soup = BeautifulSoup(html,\"html.parser\")\n",
        "  title = soup.find(\"div\",{\"class\": \"content\"}).find(\"h1\").getText() # TİTLE Tamam\n",
        "\n",
        "  date = soup.find(\"div\",{\"class\" : \"content-info\"}).find_all(\"ul\")\n",
        "  date = date[1].getText()\n",
        "  date = date[:-9]\n",
        "  date= date.strip()\n",
        "\n",
        "  content = soup.find(\"div\",{\"class\" : \"content\"}).find_all(\"p\")\n",
        "  content_string =\"\"\n",
        "  categori =\"Technology\"\n",
        "  \n",
        "  for i in content:\n",
        "    i=i.getText().replace(\" \",\" \")\n",
        "    z=\" \".join(i.split())        \n",
        "    content_string += z\n",
        "          \n",
        "\n",
        "  sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,content_string)\n",
        "  with open(\"hürriyetdaily_technology_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "    file.write(sentence +\"\\n\")"
      ],
      "metadata": {
        "id": "BXhD07YmxcR_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(creator_science_content,i.strip()) for i in science_links]\n",
        "print(time.time()-t1)\n",
        "\n",
        "t2=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(creator_technology_content,i.strip()) for i in technology_links]\n",
        "print(time.time()-t2)"
      ],
      "metadata": {
        "id": "GiK3lMLmAd3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}