# -*- coding: utf-8 -*-
"""takvim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KcAtrDU5rpQPxTJw-oN1wl5cPEATOBOo
"""

import os
import bs4
import requests
from bs4 import BeautifulSoup
import time
import urllib.request
import re
import urllib3
from pandas import DataFrame
import csv
import datetime
from datetime import datetime, timedelta
import time
import concurrent
import multiprocessing
from multiprocessing import pool

prostat_kanser_link = "https://www.takvim.com.tr/arama/arsiv?query=prostat%20kanseri&sort=createddatereal&issearchurl=true&haber=6520&video=79&galeri=116&tum=6715&yazar=0&tc=6213&page="
meme_kanser_link = "https://www.takvim.com.tr/arama/arsiv?query=meme%20kanseri&sort=createddatereal&issearchurl=true&haber=6300&video=90&galeri=132&tum=6522&yazar=0&tc=860&page="
kanser_link ="https://www.takvim.com.tr/arama/arsiv?query=kanser&sort=createddatereal&issearchurl=true&haber=12940&video=189&galeri=194&tum=13324&yazar=0&tc=7710&page="
links = [kanser_link,prostat_kanser_link,meme_kanser_link]
numbers = [772,623,568]
kanser_pages = []
prostat_pages = []
meme_kanser_pages = []

def kanser_page_count(url,num):
  for i in range(1,num,1):
    i = str(i)
    lnk =url+i
    kanser_pages.append(lnk)

def memekanser_page_count(url,num):
  for i in range(1,num,1):
    i = str(i)
    lnk =url+i
    meme_kanser_pages.append(lnk)

def prostat_kanser_page_count(url,num):
  for i in range(1,num,1):
    i = str(i)
    lnk =url+i
    prostat_pages.append(lnk)

kanser_page_count(kanser_link,numbers[0])
prostat_kanser_page_count(prostat_kanser_link,numbers[1])
memekanser_page_count(meme_kanser_link ,numbers[2])
print(prostat_pages)

def get_link(link):
  html = requests.get(link).content
  soup = BeautifulSoup(html,"html.parser")
  
  sayfa_url = soup.find("div",{"class":"searchList"}).find("ul").find_all("li")
  for i in sayfa_url:
    href = i.find("a").get("href")
    # print(href)
    if href.startswith("/"):
      href = "https://www.takvim.com.tr"+href

      
    else:
      pass
    with open("takvim_Kanser_Links.txt","a",encoding="utf-8")as file:
      file.write(href+"\n")

import concurrent
t1=time.time()
with concurrent.futures.ProcessPoolExecutor() as execut:
  b_res=[execut.submit(get_link,i.strip()) for i in kanser_pages]

print(time.time()-t1)

def get_link2(link):
  html = requests.get(link).content
  soup = BeautifulSoup(html,"html.parser")
  
  sayfa_url = soup.find("div",{"class":"searchList"}).find("ul").find_all("li")
  for i in sayfa_url:
    href = i.find("a").get("href")
    # print(href)
    if href.startswith("/"):
      href = "https://www.takvim.com.tr"+href
     
    else:
      pass
    with open("takvim_meme_KanserLinks.txt","a",encoding="utf-8")as file:
      file.write(href+"\n")

import concurrent
t1=time.time()
with concurrent.futures.ProcessPoolExecutor() as execut:
  b_res=[execut.submit(get_link2,i.strip()) for i in meme_kanser_pages]

print(time.time()-t1)

def get_link3(link):
  html = requests.get(link).content
  soup = BeautifulSoup(html,"html.parser")
  
  sayfa_url = soup.find("div",{"class":"searchList"}).find("ul").find_all("li")
  for i in sayfa_url:
    href = i.find("a").get("href")
    
    if href.startswith("/"):
      href = "https://www.takvim.com.tr"+href
     
    else:
      pass
    with open("takvim_Prostat_KanserLinks.txt","a",encoding="utf-8")as file:
      file.write(href+"\n")

import concurrent
t1=time.time()
with concurrent.futures.ProcessPoolExecutor() as execut:
  b_res=[execut.submit(get_link3,i.strip()) for i in prostat_pages]

print(time.time()-t1)

takvim_kanser_links=[]
with open("takvim_Kanser_Links.txt",'r',newline='') as f:
  for i in f.readlines():    
  # reader=csv.reader(f)
    takvim_kanser_links.append(i)

takvim_meme_kanser_links=[]
with open("takvim_meme_KanserLinks.txt",'r',newline='') as f:
  for i in f.readlines():    
  # reader=csv.reader(f)
    takvim_meme_kanser_links.append(i)

takvim_prostat_kanser_links=[]
with open("takvim_Prostat_KanserLinks.txt",'r',newline='') as f:
  for i in f.readlines():    
  # reader=csv.reader(f)
    takvim_prostat_kanser_links.append(i)

def creator1 (url):
  html = requests.get(url).content
  soup = BeautifulSoup(html,"html.parser")

  content_string = ""
  date_string = ""


  title = soup.find("h1",{"id":"haberTitle"}).getText()
  date = soup.find("div",{"class":"infoBox"}).find_all("li")
  date = date[1].getText()
  date_string = date[:-6]

  categori = soup.find("span",{"class":"categoryBox gundem"}).getText()

  content_array = soup.find("div", attrs = {"id":"contextual"}).getText().split()
  for w in content_array:
      content_string = content_string+" "+w
  sentence = "{} ; {} ; {} ; {} ; {}".format(url,date_string,categori,title,content_string)
  with open("TakvimKanserCont.txt","a",encoding="utf-8") as file :
    file.write(sentence+"\n")

def creator2 (url):
  html = requests.get(url).content
  soup = BeautifulSoup(html,"html.parser")

  content_string = ""
  date_string = ""


  title = soup.find("h1",{"id":"haberTitle"}).getText()
  date = soup.find("div",{"class":"infoBox"}).find_all("li")
  date = date[1].getText()
  date_string = date[:-6]

  categori = soup.find("span",{"class":"categoryBox gundem"}).getText()

  content_array = soup.find("div", attrs = {"id":"contextual"}).getText().split()
  for w in content_array:
      content_string = content_string+" "+w
  sentence = "{} ; {} ; {} ; {} ; {}".format(url,date_string,categori,title,content_string)
  with open("Takvim_Meme_Kanseri_Cont.txt","a",encoding="utf-8") as file :
    file.write(sentence+"\n")

def creator3 (url):
  html = requests.get(url).content
  soup = BeautifulSoup(html,"html.parser")
  content_string = ""
  date_string = ""
  title = soup.find("h1",{"id":"haberTitle"}).getText()
  date = soup.find("div",{"class":"infoBox"}).find_all("li")
  date = date[1].getText()
  date_string = date[:-6]

  categori = soup.find("span",{"class":"categoryBox gundem"}).getText()

  content_array = soup.find("div", attrs = {"id":"contextual"}).getText().split()
  for w in content_array:
      content_string = content_string+" "+w
  sentence = "{} ; {} ; {} ; {} ; {}".format(url,date_string,categori,title,content_string)
  with open("Takvim_Prostat_Kanseri_Cont.txt","a",encoding="utf-8") as file :
    file.write(sentence+"\n")

import concurrent
t1=time.time()
with concurrent.futures.ProcessPoolExecutor() as execut:
  b_res=[execut.submit(creator3,i.strip()) for i in takvim_prostat_kanser_links]

print(time.time()-t1)

import concurrent
t1=time.time()
with concurrent.futures.ProcessPoolExecutor() as execut:
  b_res=[execut.submit(creator2,i.strip()) for i in takvim_meme_kanser_links]

print(time.time()-t1)

import concurrent
t1=time.time()
with concurrent.futures.ProcessPoolExecutor() as execut:
  b_res=[execut.submit(creator3,i.strip()) for i in takvim_kanser_links]

print(time.time()-t1)

# url="https://www.takvim.com.tr/magazin/2021/07/24/girtlak-kanserine-yakalanan-sarkici-tudanya-sesini-kaybetti"
# html = requests.get(url).content
# soup = BeautifulSoup(html,"html.parser")

# content_string = ""
# date_string = ""


# title = soup.find("h1",{"id":"haberTitle"}).getText()
# date = soup.find("div",{"class":"infoBox"}).find_all("li")
# date = date[1].getText()
# date_string = date[:-6]

# categori = soup.find("span",{"class":"categoryBox gundem"}).getText()

# content_array = soup.find("div", attrs = {"id":"contextual"}).getText().split()
# for w in content_array:
#     content_string = content_string+" "+w
    

# sentence = "{} ; {} ; {} ; {} ; {}".format(url,date_string,categori,title,content_string)

# print(sentence)

# gazeteler = ["Birgün","Cumhuriyet","EnsonHaber","GazeteDuvar","Hürriyet"\
#             "karar","sabah","turkiye gazetes", "yenisafak", "takvim"]
# dizi_kanser =  [2440,478,1412,428,13970,6751,671,14545,1618,929,7260]
# dizi_prostat = [65,475,15,109,818,7394,14,12693,31,289,5993]
# dizi_meme_kanser =    [100,478,127,130,1509,7376,74,14093,95,0,5511]
# total = 0
# tamamı = 0
# for i in dizi_kanser :
#   total += i
# tamamı +=total
# print("Kanser : ",total)
# for i in dizi_prostat :
#   total += i
# tamamı +=total
# print("Prostat Kanseri : ",total)
# for i in dizi_meme_kanser :
#   total += i

# tamamı +=total
# print("Meme Kanseri : ",total)
# print("Genel Toplam :",tamamı)

# def container3(url):
#   # print(url)
#   r = requests.get(url)  
#   soup = BeautifulSoup(r.content, 'html5lib')

#   cat=soup.find("div",{"class":"category"}).a.getText()
  
#   title = soup.find("h1", attrs = {"class":"title"}).getText()
#   content_array = soup.find_all("p", attrs = {"class":"non-card"})
#   date = soup.find("time", attrs = {"class":"item time"}).getText()
#   # print(url)
#   # print(date[:-8])
#   # print(cat)
#   # print(title)
#   # date = date.split()
#   # date = date[2]
#   # cat=find_categories(url)  
#   # content_array = content_array.split()
#   content_string = ""
#   txt=""
#   for i in cont:
#     txt+=i
 
#   for w in content_array:
#     # print(w.getText())
#     content_string+=w.getText().strip()
#     # content_string = content_string+" "+w
#   # print(content_string)
#   w_data = "{};{};{};{};{}".format(url,date[:-8],cat,title,content_string)
#   print(w_data)
#   # data="{};{};{};{};{}".format(url,date,cat,title,content_string)
#   # # print(data)
#   # write_to_txt(data)
#   with open("yenisafak_contain_prostat.txt", 'a') as file: 
#        file.write(w_data+'\n')