{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "total_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7QZtQFYBf9z5MEeKFhnlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doukansurel/Kod/blob/main/total_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Fykzols3PC"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "\n",
        "import os\n",
        "import io\n",
        "import bs4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import re\n",
        "import urllib3\n",
        "from pandas import DataFrame\n",
        "import csv\n",
        "import datetime\n",
        "from datetime import datetime, timedelta ,date\n",
        "from selenium import webdriver\n",
        "import concurrent\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome(options=options)\n",
        "\n",
        "!pip install kora -q\n",
        "from kora.selenium import wd\n",
        "from selenium.webdriver.common.keys import Keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N255K-FpM0m7",
        "outputId": "0eb36bc1-0512-4f64-ba27-216c9a2b2fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent\n",
        "import multiprocessing\n",
        "from multiprocessing import pool\n",
        "import io\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "firMCi6zFtgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Telegraph:\n",
        "  technology_page_link = []\n",
        "  science_page_link = []\n",
        "  \n",
        "  def date_creator(self,technology_page_link,science_page_link):    \n",
        "    for i in range(0,222):\n",
        "      lnk = \"https://thetelegraph.newsbank.com/search?text=Science&content_added=&date_from=&date_to=&s_dlid=&s_ecproduct=SUB-FREE&s_ecprodtype=INSTANT&s_trackval=&s_siteloc=&s_referrer=&s_subterm=Subscription%20until%3A%2012/31/2080&s_docsbal=%20&s_subexpires=12/31/2080&s_docstart=&s_docsleft=&s_docsread=&s_username=freeusers&s_accountid=AC0121090715212929999&s_upgradeable=no&pub%5B0%5D=TELAL&page={}\".format(str(i))\n",
        "      science_page_link.append(lnk)\n",
        "    for i in range(0,214):\n",
        "      lnk = \"https://thetelegraph.newsbank.com/search?text=Technology&content_added=&date_from=&date_to=&s_dlid=&s_ecproduct=SUB-FREE&s_ecprodtype=INSTANT&s_trackval=&s_siteloc=&s_referrer=&s_subterm=Subscription%20until%3A%2012/31/2080&s_docsbal=%20&s_subexpires=12/31/2080&s_docstart=&s_docsleft=&s_docsread=&s_username=freeusers&s_accountid=AC0121090715212929999&s_upgradeable=no&pub%5B0%5D=TELAL&page={}\".format(str(i))\n",
        "      technology_page_link.append(lnk)\n",
        "    return technology_page_link,science_page_link\n",
        "\n",
        "  def technology_get_link_creator(self,url):\n",
        "    html = requests.get(url).content                                                          \n",
        "    soup = BeautifulSoup(html,\"html.parser\")\n",
        "    lnk_list = soup.find(\"div\",{\"class\":\"view-content\"}).find_all(\"div\",{\"class\":\"views-field views-field-text-6\"})\n",
        "    for i in lnk_list:\n",
        "      href = i.find(\"a\").get(\"href\")\n",
        "      if(href.startswith(\"https://thetelegraph.newsbank.com/\")):\n",
        "        pass\n",
        "      else: \n",
        "        href = \"https://thetelegraph.newsbank.com/\"+href\n",
        "      with open(\"telegraph_technology_Links.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "        file.write(href+\"\\n\")\n",
        "\n",
        "  def science_get_link_creator(self,url):\n",
        "    html = requests.get(url).content                                                          \n",
        "    soup = BeautifulSoup(html,\"html.parser\")\n",
        "    lnk_list = soup.find(\"div\",{\"class\":\"view-content\"}).find_all(\"div\",{\"class\":\"views-field views-field-text-6\"})\n",
        "    for i in lnk_list:\n",
        "      href = i.find(\"a\").get(\"href\")\n",
        "      if(href.startswith(\"https://thetelegraph.newsbank.com/\")):\n",
        "        pass\n",
        "      else: \n",
        "        href = \"https://thetelegraph.newsbank.com/\"+href\n",
        "      with open(\"telegraph_science_Links.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "        file.write(href+\"\\n\")\n",
        "\n",
        "  def transfer_get_links(self,technology_links,science_links):\n",
        "    with open(\"telegraph_science_Links.txt\",'r',newline='') as f:\n",
        "      for i in f.readlines():    \n",
        "      # reader=csv.reader(f)\n",
        "        i = i.strip(\"\\n\")\n",
        "        science_links.append(i)\n",
        "\n",
        "    with open(\"telegraph_technology_Links.txt\",'r',newline='') as f:\n",
        "      for i in f.readlines():    \n",
        "      # reader=csv.reader(f)\n",
        "        i = i.strip(\"\\n\")\n",
        "        technology_links.append(i) \n",
        "        \n",
        "    return technology_links,science_links\n",
        "\n",
        "  def creator_science_content(self,url):\n",
        "    wd.get(url)\n",
        "    title = wd.find_element_by_xpath('//*[@id=\"nb-bootstrap-three-column-main-content\"]/div[3]/h1').text.strip()\n",
        "    content2 = wd.find_element_by_xpath('//*[@id=\"nb-asciiviewer-news\"]/div[4]').text.strip()\n",
        "    content = wd.find_element_by_xpath('//*[@id=\"nb-asciiviewer-news\"]/div[5]').text.strip().replace(\"\\n\",\"\")\n",
        "    all_content = content2 +content\n",
        "    date = url[-10:]\n",
        "    date= date.strip() \n",
        "    categori = \"Science\"\n",
        "    sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,all_content)\n",
        "    with open(\"telegraph_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "      file.write(sentence +\"\\n\")\n",
        "  def creator_technology_content(self,url): \n",
        "    wd.get(url)\n",
        "    title = wd.find_element_by_xpath('//*[@id=\"nb-bootstrap-three-column-main-content\"]/div[3]/h1').text.strip()\n",
        "    content2 = wd.find_element_by_xpath('//*[@id=\"nb-asciiviewer-news\"]/div[4]').text.strip()\n",
        "    content = wd.find_element_by_xpath('//*[@id=\"nb-asciiviewer-news\"]/div[5]').text.strip().replace(\"\\n\",\"\")\n",
        "    all_content = content2 +content\n",
        "    date = url[-10:]\n",
        "    date= date.strip()\n",
        "    categori = \"Technology\"\n",
        "    sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,all_content)\n",
        "    with open(\"telegraph_Technology_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "      file.write(sentence +\"\\n\")    \n",
        "      "
      ],
      "metadata": {
        "id": "RrCY5jnptMs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telegraph = Telegraph()\n",
        "technology_page_link = []\n",
        "science_page_link = []\n",
        "technology_links = []\n",
        "science_links = []\n",
        "\n",
        "technology_page,science_page = telegraph.date_creator(technology_page_link,science_page_link)\n",
        "\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(telegraph.science_get_link_creator,i.strip()) for i in science_page]\n",
        "print(time.time()-t1)\n",
        "\n",
        "t2=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(telegraph.technology_get_link_creator,i.strip()) for i in technology_page]\n",
        "print(time.time()-t2)\n",
        "\n",
        "technology_total_links,science_total_links = telegraph.transfer_get_links(technology_links,science_links)\n",
        "\n",
        "t3=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(telegraph.creator_science_content,i.strip()) for i in science_total_links]\n",
        "print(time.time()-t3)\n",
        "\n",
        "t4=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(telegraph.creator_technology_content,i.strip()) for i in technology_total_links]\n",
        "print(time.time()-t4)\n",
        "\n"
      ],
      "metadata": {
        "id": "dWSg2IoVuPxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technology_page"
      ],
      "metadata": {
        "id": "VkJFdb8WMGE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Daily_Sabah:\n",
        "  technology_link = []\n",
        "  science_link = []\n",
        "  technology_wrong_link = []\n",
        "  \n",
        "  def date_creator(self,technology_link,science_link):\n",
        "    for i in range(1,234):\n",
        "      lnk = \"https://www.dailysabah.com/search?query=Technology&qlimit=by_fifty&pgno={}\".format(str(i))\n",
        "      technology_link.append(lnk)\n",
        "\n",
        "    for i in range(1,170):\n",
        "      lnk = \"https://www.dailysabah.com/search?qsubsection=science&pgno={}\".format(str(i))\n",
        "      science_link.append(lnk)\n",
        "\n",
        "    return technology_link,science_link\n",
        "  \n",
        "  def science_get_link_creator(self,url):\n",
        "    html = requests.get(url).content                                                          \n",
        "    soup = BeautifulSoup(html,\"html.parser\")\n",
        "    list = soup.find(\"div\",{\"class\" :\"list_with_rectangle_image search_results\"}).find_all(\"div\",{\"class\": \"widget_content\"})\n",
        "    for i in list:\n",
        "      href = i.find(\"h3\").find(\"a\").get(\"href\")\n",
        "      with open(\"daily_science_Links.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "        file.write(href+\"\\n\")\n",
        "\n",
        "  def transfer_get_links(self,technology_links,science_links):\n",
        "    with open(\"daily_technology_Links.txt\",'r',newline='') as f:\n",
        "      for i in f.readlines():    \n",
        "      # reader=csv.reader(f)\n",
        "        i = i.strip(\"\\n\")\n",
        "        technology_links.append(i)\n",
        "    with open(\"daily_science_Links.txt\",'r',newline='') as f:\n",
        "      for i in f.readlines():    \n",
        "      # reader=csv.reader(f)\n",
        "        i = i.strip(\"\\n\")\n",
        "\n",
        "        science_links.append(i)\n",
        "    return technology_links,science_links\n",
        "\n",
        "  def creator_science_content(self,url):\n",
        "    html = requests.get(url).content\n",
        "    soup = BeautifulSoup(html,\"html.parser\")\n",
        "    title = soup.find(\"div\",{\"class\" : \"top_title_widget\"}).find(\"h1\").getText().strip() # title tamam \n",
        "    content = soup.find(\"div\",{\"class\" : \"article_body\"})\n",
        "    date = soup.find(\"div\",{\"class\" : \"left_mobile_details\"})\n",
        "    categori = \"Science\"\n",
        "    content_string=\"\"\n",
        "    if date == None:\n",
        "      date = \"\"\n",
        "    else :\n",
        "      date =soup.find(\"div\",{\"class\" : \"left_mobile_details\"}).find_all(\"span\")\n",
        "      if(len(date) == 2 ):\n",
        "          date = date[1].getText().strip()\n",
        "          date = date[:-15].replace(\",\",\"\") \n",
        "          date = date.strip()\n",
        "      else : \n",
        "          date = date[2].getText().strip()\n",
        "          date = date[:-16].replace(\",\",\"\") \n",
        "          date = date.strip()\n",
        "      \n",
        "    if content == None : \n",
        "      sentence = \"{} ; {} ; {} ; {} ; \".format(url,date,categori,title)\n",
        "      with open(\"daily_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "        file.write(sentence+\"\\n\")\n",
        "    else:\n",
        "      content = soup.find(\"div\",{\"class\" : \"article_body\"}).find_all(\"p\")\n",
        "      for i in content:\n",
        "        i=i.getText().replace(\" \",\" \")\n",
        "        z=\" \".join(i.split())        \n",
        "        content_string += z\n",
        "            \n",
        "      if content_string != \"\":\n",
        "        content = soup.find(\"div\",{\"class\" : \"article_body\"}).getText()  \n",
        "        content = content.strip()\n",
        "        content =\" \".join(content.split())\n",
        "        content_string = content   \n",
        "\n",
        "      sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,content_string)\n",
        "      with open(\"daily_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "        file.write(sentence+\"\\n\")\n",
        "        \n",
        "  def creator_technology_content(self,url):\n",
        "    html = requests.get(url).content\n",
        "    soup = BeautifulSoup(html,\"html.parser\")\n",
        "    title = soup.find(\"div\",{\"class\" : \"top_title_widget\"}).find(\"h1\").getText().strip() # title tamam \n",
        "    content = soup.find(\"div\",{\"class\" : \"article_body\"})\n",
        "    date = soup.find(\"div\",{\"class\" : \"left_mobile_details\"})\n",
        "    categori = \"Science\"\n",
        "    content_string=\"\"\n",
        "    if date == None:\n",
        "      date = \"\"\n",
        "    else :\n",
        "      date =soup.find(\"div\",{\"class\" : \"left_mobile_details\"}).find_all(\"span\")\n",
        "      if(len(date) == 2 ):\n",
        "          date = date[1].getText().strip()\n",
        "          date = date[:-15].replace(\",\",\"\") \n",
        "          date = date.strip()\n",
        "      else : \n",
        "          date = date[2].getText().strip()\n",
        "          date = date[:-16].replace(\",\",\"\") \n",
        "          date = date.strip()\n",
        "      \n",
        "    if content == None : \n",
        "      sentence = \"{} ; {} ; {} ; {} ; \".format(url,date,categori,title)\n",
        "      with open(\"daily_Science_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "        file.write(sentence+\"\\n\")\n",
        "    else:\n",
        "      content = soup.find(\"div\",{\"class\" : \"article_body\"}).find_all(\"p\")\n",
        "      for i in content:\n",
        "        i=i.getText().replace(\" \",\" \")\n",
        "        z=\" \".join(i.split())        \n",
        "        content_string += z\n",
        "            \n",
        "      if content_string != \"\":\n",
        "        content = soup.find(\"div\",{\"class\" : \"article_body\"}).getText()  \n",
        "        content = content.strip()\n",
        "        content =\" \".join(content.split())\n",
        "        content_string = content   \n",
        "\n",
        "      sentence = \"{} ; {} ; {} ; {} ; {}\".format(url,date,categori,title,content_string)\n",
        "      with open(\"daily_technology_Cont.txt\",\"a\",encoding=\"utf-8\") as file:\n",
        "        file.write(sentence+\"\\n\")\n"
      ],
      "metadata": {
        "id": "zlONY1XK3zZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_sabah = Daily_Sabah()\n",
        "d1= []\n",
        "d2 = []\n",
        "d3 = []\n",
        "d4 = []\n",
        "technology_link,science_link = daily_sabah.date_creator(d1,d2)\n",
        "\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_sabah.science_get_link_creator,i.strip()) for i in science_link]\n",
        "print(time.time()-t1)\n",
        "\n",
        "t2=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_sabah.technology_get_link_creator,i.strip()) for i in technology_link]\n",
        "print(time.time()-t2)\n",
        "\n",
        "technology_links,science_links =daily_sabah .transfer_get_links(d3,d4)\n",
        "\n",
        "t3=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_sabah.creator_science_content,i.strip()) for i in science_links]\n",
        "print(time.time()-t3)\n",
        "\n",
        "t4=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_sabah.creator_technology_content,i.strip()) for i in technology_links]\n",
        "print(time.time()-t4)\n"
      ],
      "metadata": {
        "id": "UftRTww-4LRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Daily_Hurriyet:\n",
        "  technology_page_link = []\n",
        "  science_page_link = []\n",
        "  def date_creator(self,technology_page_link,science_page_link):\n",
        "    for i in range(1,51):\n",
        "      lnk = \"https://www.hurriyetdailynews.com/search/science?p={}\".format(str(i))\n",
        "      science_page_link.append(lnk)\n",
        "    for i in range(1,21):\n",
        "      lnk = \"https://www.hurriyetdailynews.com/search/technology?p={}\".format(str(i))\n",
        "      technology_page_link.append(lnk)\n",
        "    return technology_page_link,science_page_link\n",
        "\n",
        "  def technology_get_link_creator(self,url):\n",
        "    html = requests.get(url).content      \n",
        "    soup = BeautifulSoup(html,\"html.parser\")\n",
        "    link_list = soup.find_all(\"div\",{\"class\" : \"col-md-4 col-sm-4\"})\n",
        "    for i in link_list:\n",
        "      href = i.find(\"a\").get(\"href\")\n",
        "      if href.startswith(\"https://www.hurriyetdailynews.com\"):\n",
        "        pass\n",
        "      else:\n",
        "        href = \"https://www.hurriyetdailynews.com\"+href\n",
        "        with open(\"daily_h端rriyet_technology_Links.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "          file.write(href+\"\\n\")\n",
        "\n",
        "  def science_get_link_creator(self,url):\n",
        "    html = requests.get(url).content \n",
        "    soup = BeautifulSoup(html,\"html.parser\")\n",
        "    link_list = soup.find_all(\"div\",{\"class\" : \"col-md-4 col-sm-4\"})\n",
        "    for i in link_list:\n",
        "      href = i.find(\"a\").get(\"href\")\n",
        "      if href.startswith(\"https://www.hurriyetdailynews.com\"):\n",
        "        pass\n",
        "      else:\n",
        "        href = \"https://www.hurriyetdailynews.com\"+href\n",
        "        with open(\"daily_h端rriyet_science_Links.txt\",\"a\",encoding=\"utf-8\") as file :\n",
        "          file.write(href+\"\\n\")\n",
        "\n",
        "  def transfer_get_link(self,HurriyetTech,HurriyetScience):\n",
        "    with open(\"/content/daily_h端rriyet_science_Links.txt\",'r',newline='') as f:\n",
        "      for i in f.readlines():    \n",
        "        HurriyetScience.append(i)\n",
        "\n",
        "    with open(\"/content/daily_h端rriyet_technology_Links.txt\",'r',newline='') as f:\n",
        "      for i in f.readlines():    \n",
        "        HurriyetTech.append(i)\n",
        "\n",
        "    return HurriyetTech,HurriyetScience\n",
        "\n",
        "  def creator_tech(self,url):\n",
        "    r = requests.get(url)   \n",
        "    soup = BeautifulSoup(r.content, 'html5lib')\n",
        "    title = soup.find(\"h1\").getText()\n",
        "    content_array = soup.find(\"div\", attrs = {\"class\":\"content\"}).getText()\n",
        "    category = \"tech\"\n",
        "    date = soup.find(\"li\", attrs = {\"class\":\"date\"}).getText()\n",
        "    date = date.split()\n",
        "    date = date[0] +\"-\"+ date[1] +\"-\"+ date[2]\n",
        "    date = datetime.datetime.strptime(date,\"%B-%d-%Y\").strftime(\"%Y-%m-%d\")\n",
        "    content_array = content_array.split()\n",
        "    content_string = \"\"\n",
        "    for w in content_array:\n",
        "      content_string = content_string +\" \" + w\n",
        "    w_data=\"{};{};{};{};{}\".format(url,date,category,title,content_string)\n",
        "    with open(\"contentAll2.txt\", 'a') as file: \n",
        "        file.write(w_data+'\\n')\n",
        "\n",
        "  def creator_science(self,url):\n",
        "    r = requests.get(url)   \n",
        "    soup = BeautifulSoup(r.content, 'html5lib')\n",
        "    title = soup.find(\"h1\").getText()\n",
        "    content_array = soup.find(\"div\", attrs = {\"class\":\"content\"}).getText()\n",
        "    category = \"science\"\n",
        "    date = soup.find(\"li\", attrs = {\"class\":\"date\"}).getText()\n",
        "    date = date.split()\n",
        "    date = date[0] +\"-\"+ date[1] +\"-\"+ date[2]\n",
        "    date = datetime.datetime.strptime(date,\"%B-%d-%Y\").strftime(\"%Y-%m-%d\")\n",
        "    content_array = content_array.split()\n",
        "    content_string = \"\"\n",
        "    for w in content_array:\n",
        "      content_string = content_string +\" \" + w\n",
        "    w_data=\"{};{};{};{};{}\".format(url,date,category,title,content_string)\n",
        "    with open(\"contentAll2.txt\", 'a') as file: \n",
        "        file.write(w_data+'\\n')"
      ],
      "metadata": {
        "id": "5gQUREsd4p9O"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_hurriyet = Daily_Hurriyet()\n",
        "d1 = []\n",
        "d2 = []\n",
        "d3 = []\n",
        "d4 = []\n",
        "\n",
        "technology_page_link,science_page_link =  daily_hurriyet.date_creator(d1,d2)\n",
        "\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_hurriyet.science_get_link_creator,i.strip()) for i in science_page_link]\n",
        "print(time.time()-t1)\n",
        "\n",
        "t2=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_hurriyet.technology_get_link_creator,i.strip()) for i in technology_page_link]\n",
        "print(time.time()-t2)\n",
        "\n",
        "HurriyetTech,HurriyetScience = daily_hurriyet.transfer_get_link(d3,d4)\n",
        "\n",
        "t3=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_hurriyet.creator_science,i.strip()) for i in HurriyetScience]\n",
        "print(time.time()-t3)\n",
        "\n",
        "t4=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(daily_hurriyet.creator_tech,i.strip()) for i in HurriyetTech]\n",
        "\n",
        "print(time.time()-t4)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WM3IB1dVBkwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IndependentNews:\n",
        "  lnk = []\n",
        "  def datecreator(self,lnk):\n",
        "    ser_date = pd.Series(pd.date_range('19920101', periods=12000))\n",
        "    link=\"https://www.independent.co.uk/archive/\"\n",
        "\n",
        "    for i in range(0,10958): #10958\n",
        "        dateEnd  = ser_date[i].strftime(\"%Y-%m-%d\")\n",
        "        lnk.append(\"{}{}\".format(link,dateEnd))\n",
        "    return lnk\n",
        "\n",
        "  def get_link(self,i):\n",
        "     print(i)\n",
        "     r = requests.get(i)   \n",
        "     soup = BeautifulSoup(r.content, 'html5lib')\n",
        "     for a in soup.find_all('a', href=True):\n",
        "       link = a['href']\n",
        "       result = link.endswith(\".html\")\n",
        "       if (result==True):\n",
        "         result2 = link.startswith(\"/service\") or link.startswith(\"/news/world/journalism-license-srmg-middle-east-news-world-global-a9579111.html\")\n",
        "         if (result2==False):\n",
        "           ekle=\"https://www.independent.co.uk\"\n",
        "           link=\"{}{}\".format(ekle,link)\n",
        "           with open(\"independent.txt\", 'a') as file: \n",
        "               file.write(link+'\\n')\n",
        "\n",
        "  def creator(self,url):\n",
        "    r = requests.get(url)   \n",
        "    soup = BeautifulSoup(r.content, 'html5lib')\n",
        "    title = soup.find(\"h1\").getText()\n",
        "    content_array = soup.find(\"div\", attrs = {\"class\":\"sc-jcZdLI eClmBz sc-jTSbJK cMrSbI\"}).getText()\n",
        "    # print (title)\n",
        "    date = soup.find(\"div\", attrs = {\"class\":\"sc-LEJYB jHqZxO\"}).getText()\n",
        "    date = date.split()\n",
        "    date = date[3] +\"-\"+ date[2] +\"-\"+ date[1] \n",
        "    # print (date)\n",
        "    date = datetime.datetime.strptime(date,\"%Y-%B-%d\").strftime(\"%Y-%m-%d\")\n",
        "    # print (date)\n",
        "    content_array = content_array.split()\n",
        "    content_string = \"\"\n",
        "    for w in content_array:\n",
        "      stop = \"(function({\"\n",
        "      if(w == stop):\n",
        "        break\n",
        "      else:\n",
        "        content_string = content_string+\" \"+w\n",
        "        #w_data = url+\";\"+date+\";\"+title+\";\"+content_string\n",
        "\n",
        "    w_data=\"{};{};{};{}\".format(url,date,title,content_string)\n",
        "\n",
        "    # write_to_txt(w_data)\n",
        "    with open(\"IndependentContent2.txt\", 'a') as file: \n",
        "        file.write(w_data+'\\n')\n",
        "\n",
        "  def transfer_get_links(self,Linknews):\n",
        "    with open(\"independent.txt\",'r',newline='') as f:\n",
        "      for i in f.readlines():    \n",
        "        Linknews.append(i)\n",
        "    return Linknews\n",
        "\n",
        "  def independent_main(self):\n",
        "    independent = IndependentNews()\n",
        "    d1 = []\n",
        "    d2 = []\n",
        "    lnks = independent.datecreator(d1)\n",
        "    Linknews = []\n",
        "\n",
        "    t1=time.time()\n",
        "    with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "      b_res=[execut.submit(independent.get_link,i.strip()) for i in lnks]\n",
        "    print(time.time()-t1)\n",
        "\n",
        "    Linknews = independent.transfer_get_links(d2)\n",
        "    t2=time.time()\n",
        "    with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "      b_res=[execut.submit(independent.creator,i.strip()) for i in Linknews]\n",
        "    print(time.time()-t2)\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "v5rE5-i1JV6I"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "independent = IndependentNews()\n",
        "independent.independent_main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLbyUlFTNfTA",
        "outputId": "ef806d88-6999-4a40-9e36-e11aaea5ea84"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.independent.co.uk/archive/2005-09-09\n",
            "0.32541918754577637\n",
            "22.472647428512573\n"
          ]
        }
      ]
    }
  ]
}